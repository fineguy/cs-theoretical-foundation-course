\input{../base.tex}
\graphicspath{ {imgs/} }

\begin{document}

%==================================================
\mynonumbersection{ЧИСЛЕННЫЕ МЕТОДЫ ОПТИМИЗАЦИИ}

%==================================================
\mysection{Численные методы оптимизации}

%========================================
\mysubsection{Основные определения}

Пусть задано множество $Х$ и функция $f(x)$ , определенная на этом множестве. Требуется найти точки минимума или максимума функции $f(x)$ на множестве $Х$. Задачу на минимум записывают следующим образом:

\myequation{
f(x) \rightarrow \min, x \in X
}
\

При этом функцию $f(x)$ называют целевой функцией; множество $Х$ – допустимым множеством; любое $x \in X$ -- допустимой точкой.

Ниже будем рассматривать так называемые конечномерные задачи оптимизации, то есть задачи допустимое множество $Х$ которых является подмножеством евклидового пространства $E^n$.

\mydefinition{
Точка $x^* \in X$ называется:
\begin{itemize}
  \item точкой глобального минимума функции $f(x)$ на множестве $Х$, если
  \myequation{
  f(x^*) \leq f(x) \; \forall x \in X
  }
  \item точкой локального минимума функции $f(x)$ на множестве $Х$, если
  \myequation{
  f(x^*) \leq f(x) \; \forall x \in X \cap U_\epsilon (x^*)
  }
\end{itemize}
}
\

Отметим, что глобальный минимум всегда одновременно является локальным, но не наоборот.

Если в (2) и (3) при $x \neq x^*$ имеем строгие неравенства, то точка $x^*$ называется соответственно точкой строгого глобального минимума, строгого локального минимума.

\mydefinition{
Вектор $h$ называется направлением убывания функции $f(x)$ в точке $x$, если выполняется

\myequation{
f(x + \alpha h) < f(x)
}
\

при всех достаточно малых $\alpha > 0$.
}
\

Множество всех направлений убывания функции $f(x)$ в точке $x$ обозначим $U(x, f )$

\mytheorem{
Пусть функция $f(x)$, дифференцируемая в точке $x \in E^n$. Если вектор $h$ является направлением убывания функции $f(x)$ в точке $x$, то справедливо

\myequation{
\langle \nabla f(x), h \rangle \leq 0
}
\

Если при некотором $h \in E^n$ выполняется

\myequation{
\langle \nabla f(x), h \rangle < 0
}
\

тогда вектор $h$ является направлением убывания функции $f(x)$ в точке $x$.
}
\

\myproof{
\textbf{Необходимость.} Пусть вектор $h \in U(x, f)$ и при этом $\langle \nabla f(x), h \rangle > 0$. В этом случае имеем

\myequation{
f(x + \alpha h) - f(x) = \langle \nabla f(x), \alpha h \rangle + O(\alpha) = \alpha \bigg( \langle \nabla f(x), h \rangle + \frac{O(\alpha)}{\alpha} \bigg) > 0
}
\

Здесь неравенство следует из того, что при достаточно малых $\alpha > 0$ знак выражения определяется знаком первого слагаемого. Таким образом, получили противоречие c (4), что и доказывает справедливость (5)

\textbf{Достаточность.} Пусть выполняется (6). В этом случае

\myequation{
f(x + \alpha h) - f(x) = \alpha \bigg( \langle \nabla f(x), h \rangle + \frac{O(\alpha)}{\alpha} \bigg) < 0
}
\

при всех достаточно малых $\alpha > 0$. Следовательно, вектор $h$ является направлением убывания функции $f(x)$ в точке $x$, то есть $h \in U(x, f)$.

}
\

\mydefinition{
Вектор $h \in E^n$ задает возможное направление относительно множества $Х$ в точке $x \in X$, если $x + \alpha h \in X$ при всех достаточно малых $\alpha > 0$ . Вектор $h$ в этом случае будем называть возможным направлением в точке x относительно множества $X$. Множество всех таких векторов $h$ обозначим через $V(x, X)$ (множество возможных направлений в точке $x \in X$).
}
\

\mytheorem{
(необходимое условие локальной оптимальности). Если $x^*$ -- локальное решение задачи (1), то

\myequation{
U(x^*, f) \cap V(x^*, X) = \O
}
}
\

\myproof{
Пусть $x^* \in X$ локальное решение (1) и при этом (9) неверно, то есть существует вектор $h \in E^n$ такой, что $f(x^* + \alpha h) < f(x^*)$ и при этом $x^* + \alpha h \in X$ при всех достаточно малых $\alpha > 0$. А это означает, что в любой сколь угодно малой окрестности точки $x^*$ существует точка $x = x^* + \alpha h \in X \cap U_\epsilon (x^*)$ такая, что справедливо $f(x) < f(x^*)$, что противоречит определению локального минимума.
}

%==================================================
\mysection{Методы безусловной оптимизации}

Рассмотрим задачу оптимизации:

\myequation{
f(x) \rightarrow \min, x \in E^n
}
\

\mytheorem{
(необходимое условие). Пусть функция $f(x)$ является дифференцируемой в точке $x^* \in E^n$. Если $x^*$ -- локальное решение задачи (10), то

\myequation{
\nabla f(x^*) = 0
}
}
\

\mydefinition{
Точка $x^*$, удовлетворяющая условию (11), называется стационарной точкой функции $f(x)$.
}
\

\mytheorem{
(необходимое условие оптимальности 2-го порядка). Пусть функция $f(x)$ дважды дифференцируемая в точке $x^* \in E^n$. Если $x^*$ -- локальное решение задачи (10), то матрица Гессе функции $f(x)$ в точке $x^*$ неотрицательно определена, то есть

\myequation{
\langle \nabla^2 f(x^*)h, h \rangle \geq 0 \; \forall h \in E^n
}
}
\

\mytheorem{
(достаточное условие). Пусть функция $f(x)$ дважды дифференцируемая в точке $x^* \in E^n$. И пусть в этой точке выполняется условие стационарности (11) и матрица Гессе положительно определена, то есть

\myequation{
\langle \nabla^2 f(x^*)h, h \rangle > 0 \; \forall h \in E^n
}
\

Тогда $x^*$ -- строгое локальное решение задачи (10).
}
\

%========================================
\mysubsection{Метод наискорейшего спуска}

Общая схема методов спуска, в которых последовательность приближений $x^1, x^2, \ldots$ к точке минимума строится по правилу:

\myequation{
x^{k+1} = x^k + \alpha_k h^k
}
\

где направление $h^k$ принадлежит множеству направлений убывания функции $h^k \in U(x^k, f); \alpha_k \geq 0$ -- параметр, определяющий длину шага вдоль направления $h^k$.

В градиентных методах направление $h^k$ берется равным антиградиенту функции $f(x)$ в точке $x^k$, то есть $h^k = - \nabla f(x^k)$. В градиентных методах используются различные методы выбора шага $\alpha_k$. Если длина шага выбирается из минимизации функции вдоль направления антиградиента, то получаем вариант градиентного метода, называемый методом наискорейшего спуска.

Итак, в методе наискорейшего спуска шаг $\alpha_k$ вдоль направления $h^k$ выбирается из решения оптимизационной задачи:

\myequation{
f(x^k - \alpha_k \nabla f(x^k)) = \min_{\alpha \geq 0} f(x^k - \alpha_k \nabla f(x^k))
}
\

\mytheorem{
Пусть функция $f(x)$ дифференцируема на всем пространстве $E^n$, и ее градиент удовлетворяет условию Липшица:

\myequation{
|| \nabla f(x + \Delta x) - \nabla f(x) || \leq L || \Delta x ||, \; \forall x, x + \Delta x \in E^n
}
\

Тогда для для остаточного члена в разложении $\Delta f(x) = \langle \nabla f(x), \Delta x \rangle + O(|| \Delta x ||)$ справедлива оценка:

\myequation{
O(|| \Delta x ||) \leq \frac{L}{2} || \Delta x || ^ 2
}
}
\

\myproof{
Для любых $x, x + \Delta x \in E^n$ справедливо соотношение

\myequation{
\Delta f(x) = f(x + \Delta x) - f(x) = \int_0^1 \langle \nabla f(x + \alpha \Delta x), \Delta x \rangle d \alpha
}
\

где $\alpha \in [0, 1]$. Действительно, рассмотрим функцию переменной $\alpha : g(\alpha) = f(x + \alpha \Delta x)$. Её производная по переменной $\alpha$ имеет следующий вид:

\myequation{
\frac{d g}{d \alpha} (\alpha) = \langle \nabla f(x + \alpha \Delta x), \Delta x \rangle
}
\

а отсюда следует справедливость

\myequation{
\int_0^1 \frac{d g}{d \alpha} (\alpha) = g(1) - g(0) = f(x + \Delta x) - f(x)
}
\

Таким образом можем представить

\myequation{
\Delta f(x) = \int_0^1 \langle \nabla f(x + \alpha \Delta x), \Delta x \rangle d \alpha = \\
=  \int_0^1 \langle \nabla f(x), \Delta x \rangle d \alpha + \int_0^1 \langle \nabla f(x + \alpha \Delta x) - \nabla f(x), \Delta x \rangle d \alpha \leq \\
\leq \int_0^1 \langle \nabla f(x), \Delta x \rangle d \alpha + \int_0^1  || f(x + \alpha \Delta x) - \nabla f(x) || \cdot || \Delta x || d \alpha
}
\

Так как градиент функции $f(x)$ удовлетворяет условию Липшица

\myequation{
\Delta f(x) \leq \langle \nabla f(x), \Delta x \rangle + \int_0^1  || f(x + \alpha \Delta x) - \nabla f(x) || \cdot || \Delta x || d \alpha \leq \\
\leq \langle \nabla f(x), \Delta x \rangle + L \int_0^1  || \alpha \Delta x || \cdot || \Delta x || d \alpha = \langle \nabla f(x), \Delta x \rangle + L || \Delta x ||^2 \int_0^1  \alpha d \alpha = \\
= \langle \nabla f(x), \Delta x \rangle + \frac{L}{2} || \Delta x ||^2 
}
\

Таким образом имеем

\myequation{
\Delta f(x) = \langle \nabla f(x), \Delta x \rangle + O(|| \Delta x ||) \leq \langle \nabla f(x), \Delta x \rangle + \frac{L}{2} || \Delta x ||^2
}
}

%========================================
\mysubsection{Метод Ньютона}

Метод Ньютона решения задач безусловной минимизации относится к методам 2-го порядка, то есть к методам, используемым информацию о вторых производных целевой функции $f(x)$.

Предположим, что функция $f(x)$ дважды непрерывно дифференцируема в $E^n$. Пусть начальное приближение $x^0 \in E^n$ задано и с помощью метода Ньютона уже найдено $k$-ое приближение $x^k \in E^n$ . В некоторой окрестности точки $x^k$ функцию $f(x)$ аппроксимируем квадратичной функцией $\psi_k(x)$:

\myequation{
\psi_k(x) = f(x^k) + \langle \nabla f(x^k), x - x^k \rangle + \frac{1}{2} \langle \nabla^2 f(x^k) \cdot [x - x^k], x - x^k \rangle
}
\

Рассмотрим вспомогательную задачу минимизации функции $\psi_k(x)$:

\myequation{
\psi_k(x) \rightarrow \min, x \in E^n
}
\

Предположим, что решение задачи (25) $\tilde{x}^k$ существует. Очевидно, что в этом случае в точке $\tilde{x}^k$ выполняется $\nabla \psi_k (\tilde{x}^k) = 0$. Так как $\nabla \psi_k (x) = \nabla f(x^k) + \nabla^2 f(x^k) \cdot [x - x^k]$, то имеем:

\myequation{
\tilde{x}^k = x^k - [\nabla^2 f(x^k)]^{-1} \nabla f(x^k)
}
\

Таким образом в качестве направления спуска можно принять:

\myequation{
h^k = - [\nabla^2 f(x^k)]^{-1} \nabla f(x^k)
}
\

Рассмотрим точки, лежащие на отрезке $[x^k, \tilde{x}^k] : x^k(\alpha) = x^k + \alpha (\tilde{x}^k - x^k)$, где $\alpha \in [0, 1]$. Здесь заметим, что в силу (26): $\tilde{x}^k - x^k = - [\nabla^2 f(x^k)]^{-1} \nabla f(x^k)$. Конкретную точку из отрезка $[x^k, \tilde{x}^k]$ выберем, найдя из условия минимума функции $f(x^k(\alpha))$. Следующее приближение определим по формуле $x^{k+1} = x^k(\alpha_k)$. С учётом решения вспомогательной задачи (25) схема метода Ньютона примет вид:

\myequation{
\begin{cases}
x^{k+1} = x^k - \alpha_k [\nabla^2 f(x^k)]^{-1} \nabla f(x^k), \\
\alpha_k: f(x^k - \alpha_k [\nabla^2 f(x^k)]^{-1} \nabla f(x^k)) \rightarrow \min, \alpha \in [0, 1]
\end{cases}
}
\

%========================================
\mysubsection{Методы сопряженных направлений}

Рассмотри задачу минимизации квадратичной функции:

\myequation{
f(x) = \frac{1}{2} \langle Ax, x \rangle + \langle b, x \rangle \rightarrow \min, x \in E^n
}
\

где $A$ - симметричная положительно определенная $n \times n$ матрица.

Идея методов сопряженных направлений основана на стремлении найти минимум квадратичной функции (29) за конечное число шагов. Согласно методу, требуется найти направления $h^0, h^1, \ldots, h^{n-1}$ такие, что последовательная одномерная минимизация функции $f(x)$ вдоль этих направлений, начиная с любой точки $x^0 \in E^n$:

\myequation{
\begin{cases}
f(x^k + \alpha_k h^k) = \min_{\lambda_k} f(x^k + \lambda_k h^k), \\
x^{k+1} = x^k + \alpha_k h^k, (k = 0, 1, \ldots, n - 1)
\end{cases}
}
\

приводит к отысканию минимума функции (29).

\mydefinition{
Вектора $h^1$ и $h^2$ называются сопряженными (относительно матрицы $А$), если они отличны от нуля и скалярное произведение $\langle A h^1, h^2 \rangle = 0$.
}
\

\mydefinition{
Вектора $h^0, h^1, \ldots, h^{k-1}$ называются взаимно сопряженными (относительно матрицы $А$), если все они отличны от нуля и скалярное произведение $\langle A h^i, h^j \rangle = 0 \; \forall i \neq j, 0 \leq i, j \leq k$.
}
\

\mytheorem{
Пусть вектора $h^0, h^1, \ldots, h^{k-1}$ являются взаимно сопряженными, тогда они линейно не зависимы.
}
\

\myproof{
Доказательство проведем от противного. Пусть вектора $h^0, h^1, \ldots, h^{k-1}$ являются взаимно сопряженными, но при этом они являются линейно зависимыми, то есть в этом случае один из векторов можно представить в виде линейной комбинации остальных векторов, например $h^i = \sum_{j=0, j \neq i}^{k-1} \lambda_j h^j$. Тогда $\langle A h^i, h^i \rangle = \sum_{j=0, j \neq i}^{k-1} \lambda_j \langle A h^i, h^j \rangle$. И отсюда в силу взаимной сопряженной векторов $\langle A h^i, h^i \rangle = 0$, что возможно, если вектор $h^i = 0$, так как матрица является по условию симметричной положительно определенной $n \times n$ матрицей. Получили противоречие с тем, что по условию взаимной сопряженности векторов $h^i \neq 0$.
}
\

\mydefinition{
Если в методе минимизации функции $f(x)$ (29) вектора $h^0, h^1, \ldots, h^{k-1}$ взаимно сопряжены, то метод (30) называется методом сопряженных направлений
}
\

\mytheorem{
Если в методе минимизации (30) функции $f(x)$ вектора $h^0, h^1, \ldots, h^{k-1}$ взаимно сопряжены, то для функции $f(x)$, заданной формулой (29) справедливо $f(x^m) = \min_{x \in X_m} f(x)$, где $X_m = x^0 + lin \; h^0, h^1, \ldots, h^{m-1}$ линейное подпространство, натянутое на указанные векторы.
}
\

\myproof{
Предварительно заметим справедливость соотношения

\myequation{
f(x^k + \lambda_k h^k) - f(x^k) = \lambda_k \langle A x^0 + b, h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle
}
\

при любом $k=0, 1, \ldots, m-1$. Действительно, учитывая, что справедливо $x^k = x^0 + \sum_{i=0}^{k-1} \alpha_i h^i$ и $\langle A h^i, h^k \rangle = 0$, получим

\myequation{
f(x^k + \lambda_k h^k) = \frac{1}{2} \langle A (x^k + \lambda_k h^k), (x^k + \lambda_k h^k) \rangle + \langle b, (x^k + \lambda_k h^k) \rangle = \\
= \frac{1}{2} \langle A x^k, x^k \rangle + \langle b, x^k \rangle + \lambda_k \langle A x^k + b, h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle = \\
= f(x^k) + \lambda_k \langle A x^0 + \sum_{i=0}^{k-1} \alpha_i A h^i + b, h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle = \\
= f(x^k) + \lambda_k \langle A x^0 + b, h^k \rangle + \lambda_k \langle \sum_{i=0}^{k-1} \alpha_i A h^i , h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle = \\
= f(x^k) + \lambda_k \langle A x^0 + b, h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle
}
\

а отсюда следует справедливость (31).

Для любой точки $x \in X_m$ имеем

\myequation{
f(x) = f(x^0 + \sum_{k=0}^{m-1} \lambda_k h^k = \\
= \frac{1}{2} \langle A (x^0 + \sum_{k=0}^{m-1} \lambda_k h^k), x^0 + \sum_{k=0}^{m-1} \lambda_k h^k \rangle + \langle b, x^0 + \sum_{k=0}^{m-1} \lambda_k h^k \rangle = \\
= f(x^0) + \sum_{k=0}^{m-1} \bigg( \lambda_k \langle A x^0 + b, h^k \rangle + \frac{1}{2} \lambda_k^2 \langle A h^k, h^k \rangle \bigg)
}
\

Отсюда, с учетом (31) для любой точки $x \in X_m$ справедливо $f(x) = f(x^0) + \sum_{k=0}^{m-1} \bigg( f(x^k + \lambda_k h^k) - f(x^k) \bigg)$. Учитывая изложенное, получим

\myequation{
\min_{x \in X_m} f(x) = \min_{\lambda^0, \ldots, \lambda^{m-1}} f(x^0 + \sum_{k=0}^{m-1} \lambda_k h^k) = \\
= \min_{\lambda^0, \ldots, \lambda^{m-1}} \bigg( f(x^0) + \sum_{k=0}^{m-1} \bigg( f(x^k + \lambda_k h^k) - f(x^k) \bigg) \bigg) = \\
= f(x^0) + \sum_{k=0}^{m-1} \bigg( \min_{\lambda_k} f(x^k + \lambda_k h^k) - f(x^k) \bigg) = \\
= f(x^0) + \sum_{k=0}^{m-1} \bigg( f(x^k + \alpha_k h^k) - f(x^k) \bigg) = \\
= f(x^0) + \sum_{k=0}^{m-1} \bigg( f(x^{k+1}) - f(x^k) \bigg) = f(x^m)
}
\

Таким образом справедливо $f(x^m) = \min_{x \in X_m} f(x)$. 
}

\end{document}

